{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e13a0b2-123c-46f9-9a44-e56eb3f37383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (404134, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th>length</th>\n",
       "      <th>info</th>\n",
       "      <th>transmission_rate_per_1000_ms</th>\n",
       "      <th>reception_rate_per_1000_ms</th>\n",
       "      <th>transmission_average_per_sec</th>\n",
       "      <th>reception_average_per_sec</th>\n",
       "      <th>transmission_count_per_sec</th>\n",
       "      <th>reception_count_per_sec</th>\n",
       "      <th>transmission_total_duration_per_sec</th>\n",
       "      <th>reception_total_duration_per_sec</th>\n",
       "      <th>dao</th>\n",
       "      <th>dis</th>\n",
       "      <th>dio</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037</td>\n",
       "      <td>39</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.671176</td>\n",
       "      <td>0.539313</td>\n",
       "      <td>0.570032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037</td>\n",
       "      <td>39</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.649873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.649873</td>\n",
       "      <td>0.264704</td>\n",
       "      <td>0.530547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038</td>\n",
       "      <td>39</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671176</td>\n",
       "      <td>0.652361</td>\n",
       "      <td>0.462516</td>\n",
       "      <td>0.501327</td>\n",
       "      <td>0.671768</td>\n",
       "      <td>0.652361</td>\n",
       "      <td>0.546376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.690115</td>\n",
       "      <td>Blackhole</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045</td>\n",
       "      <td>39</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634105</td>\n",
       "      <td>0.585425</td>\n",
       "      <td>0.553276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046</td>\n",
       "      <td>39</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630378</td>\n",
       "      <td>0.443171</td>\n",
       "      <td>0.615377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time  source  destination  length  info  transmission_rate_per_1000_ms  \\\n",
       "0  0.037      39         9999     0.0   1.0                       0.000000   \n",
       "1  0.037      39         9999     0.0   1.0                       0.000000   \n",
       "2  0.038      39         9999     0.0   1.0                       0.671176   \n",
       "3  0.045      39         9999     0.0   1.0                       0.000000   \n",
       "4  0.046      39         9999     0.0   1.0                       0.000000   \n",
       "\n",
       "   reception_rate_per_1000_ms  transmission_average_per_sec  \\\n",
       "0                    0.671176                      0.000000   \n",
       "1                    0.649873                      0.000000   \n",
       "2                    0.652361                      0.462516   \n",
       "3                    0.633786                      0.000000   \n",
       "4                    0.630378                      0.000000   \n",
       "\n",
       "   reception_average_per_sec  transmission_count_per_sec  \\\n",
       "0                   0.499879                    0.000000   \n",
       "1                   0.505234                    0.000000   \n",
       "2                   0.501327                    0.671768   \n",
       "3                   0.517346                    0.000000   \n",
       "4                   0.538789                    0.000000   \n",
       "\n",
       "   reception_count_per_sec  transmission_total_duration_per_sec  \\\n",
       "0                 0.671176                             0.539313   \n",
       "1                 0.649873                             0.264704   \n",
       "2                 0.652361                             0.546376   \n",
       "3                 0.634105                             0.585425   \n",
       "4                 0.630378                             0.443171   \n",
       "\n",
       "   reception_total_duration_per_sec  dao  dis       dio   category  label  \n",
       "0                          0.570032  0.0  0.0  0.000000     Normal      0  \n",
       "1                          0.530547  0.0  0.0  0.000000     Normal      0  \n",
       "2                          1.000000  0.0  0.0  0.690115  Blackhole      1  \n",
       "3                          0.553276  0.0  0.0  0.000000     Normal      0  \n",
       "4                          0.615377  0.0  0.0  0.000000     Normal      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "data = pd.read_csv('blackhole.csv')\n",
    "print(f'Dataset shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d693dd29-b66f-4582-8088-b8e97eb3d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                                   float64\n",
      "source                                   int64\n",
      "destination                              int64\n",
      "length                                 float64\n",
      "info                                   float64\n",
      "transmission_rate_per_1000_ms          float64\n",
      "reception_rate_per_1000_ms             float64\n",
      "transmission_average_per_sec           float64\n",
      "reception_average_per_sec              float64\n",
      "transmission_count_per_sec             float64\n",
      "reception_count_per_sec                float64\n",
      "transmission_total_duration_per_sec    float64\n",
      "reception_total_duration_per_sec       float64\n",
      "dao                                    float64\n",
      "dis                                    float64\n",
      "dio                                    float64\n",
      "category                                object\n",
      "label                                    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "normal = data[data['label'] == 0]\n",
    "blackhole = data[data['label'] == 1]\n",
    "\n",
    "# Upsample minority class (blackhole)\n",
    "blackhole_upsampled = resample(blackhole, \n",
    "                               replace=True,    # sample with replacement\n",
    "                               n_samples=len(normal), # match number in majority class\n",
    "                               random_state=42) # reproducible results\n",
    "\n",
    "# Combine majority and upsampled minority\n",
    "# Identify and inspect categorical columns\n",
    "print(blackhole_upsampled.dtypes)\n",
    "\n",
    "# If there are any categorical columns, convert them to numerical using one-hot encoding\n",
    "upsampled_encoded = pd.get_dummies(blackhole_upsampled, drop_first=True)\n",
    "\n",
    "# Preprocessing: feature scaling after encoding\n",
    "X = upsampled_encoded.drop('label', axis=1)\n",
    "y = upsampled_encoded['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5578140-6325-453e-bade-d39ad53edd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Round 2\n",
      "Round 3\n",
      "Round 4\n",
      "Round 5\n",
      "Round 6\n",
      "Round 7\n",
      "Round 8\n",
      "Round 9\n",
      "Round 10\n"
     ]
    }
   ],
   "source": [
    "class MAMLModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.fc1 = layers.Dense(64, activation='relu')\n",
    "        self.fc2 = layers.Dense(32, activation='relu')\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Model training setup\n",
    "def maml_train_step(model, optimizer, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X)\n",
    "        predictions = tf.squeeze(predictions)  # Ensure predictions have the same shape as y\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Federated Learning setup\n",
    "def federated_train(X_splits, y_splits, num_rounds=10):\n",
    "    global_model = MAMLModel()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}\")\n",
    "        for X, y in zip(X_splits, y_splits):\n",
    "            maml_train_step(global_model, optimizer, X, y)\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Simulate distributed datasets for federated learning\n",
    "X_splits = np.array_split(X_train, 5)\n",
    "y_splits = np.array_split(y_train, 5)\n",
    "\n",
    "# Federated training\n",
    "global_model = federated_train(X_splits, y_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c738bb1-1dc0-4e49-bf0f-97e6fa7b4e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[53971]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     53971\n",
      "\n",
      "    accuracy                           1.00     53971\n",
      "   macro avg       1.00      1.00      1.00     53971\n",
      "weighted avg       1.00      1.00      1.00     53971\n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_pred = global_model(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552f1bd0-2af1-47ed-a311-fd0783a2399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority class\n",
    "normal = data[data['label'] == 0]\n",
    "blackhole = data[data['label'] == 1]\n",
    "\n",
    "blackhole_upsampled = resample(blackhole, \n",
    "                               replace=True,    \n",
    "                               n_samples=len(normal),\n",
    "                               random_state=42)\n",
    "\n",
    "# Combine and encode\n",
    "upsampled_encoded = pd.get_dummies(pd.concat([normal, blackhole_upsampled]), drop_first=True)\n",
    "\n",
    "# Preprocessing\n",
    "X = upsampled_encoded.drop('label', axis=1)\n",
    "y = upsampled_encoded['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdea1f0-bfe4-4c06-9804-9d7df8d760ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Round 2\n",
      "Round 3\n",
      "Round 4\n",
      "Round 5\n",
      "Round 6\n",
      "Round 7\n",
      "Round 8\n",
      "Round 9\n",
      "Round 10\n"
     ]
    }
   ],
   "source": [
    "# Define MAML model with Dropout\n",
    "class MAMLModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.fc1 = layers.Dense(64, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(0.5)  # Dropout layer\n",
    "        self.fc2 = layers.Dense(32, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.5)  # Dropout layer\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "        return self.out(x)\n",
    "\n",
    "# Model training setup\n",
    "def maml_train_step(model, optimizer, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X)\n",
    "        predictions = tf.squeeze(predictions)\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Federated Learning setup\n",
    "def federated_train(X_splits, y_splits, num_rounds=10):\n",
    "    global_model = MAMLModel()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}\")\n",
    "        for X, y in zip(X_splits, y_splits):\n",
    "            maml_train_step(global_model, optimizer, X, y)\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Simulate distributed datasets for federated learning\n",
    "X_splits = np.array_split(X_train, 5)\n",
    "y_splits = np.array_split(y_train, 5)\n",
    "\n",
    "# Federated training\n",
    "global_model = federated_train(X_splits, y_splits)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = global_model(X_test)\n",
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9bde3c-600d-4d14-ba41-bc4b92b2b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [107941, 53971]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:342\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    248\u001b[0m     {\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [107941, 53971]"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "315a3c4b-6373-4263-87ea-106e4fba7723",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Assuming global_model and scaler are available\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mget_user_input_and_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m, in \u001b[0;36mget_user_input_and_predict\u001b[1;34m(model, scaler)\u001b[0m\n\u001b[0;32m     17\u001b[0m user_input \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_names:\n\u001b[1;32m---> 19\u001b[0m     user_input[feature] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter value for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfeature\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create a DataFrame from user input\u001b[39;00m\n\u001b[0;32m     22\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(user_input, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1251\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1295\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to get user input and check the model\n",
    "def get_user_input_and_predict(model, scaler):\n",
    "    # Define feature names\n",
    "    feature_names = [\n",
    "        'time', 'source', 'destination', 'length', 'info',\n",
    "        'transmission_rate_per_1000_ms', 'reception_rate_per_1000_ms',\n",
    "        'transmission_average_per_sec', 'reception_average_per_sec',\n",
    "        'transmission_count_per_sec', 'reception_count_per_sec',\n",
    "        'transmission_total_duration_per_sec', 'reception_total_duration_per_sec',\n",
    "        'dao', 'dis', 'dio', 'category'\n",
    "    ]\n",
    "\n",
    "    # Get user input\n",
    "    user_input = {}\n",
    "    for feature in feature_names:\n",
    "        user_input[feature] = input(f\"Enter value for {feature}: \")\n",
    "\n",
    "    # Create a DataFrame from user input\n",
    "    input_data = pd.DataFrame(user_input, index=[0])\n",
    "\n",
    "    # Call the check_model function to make predictions\n",
    "    predicted_labels = check_model(model, input_data, scaler)\n",
    "    \n",
    "    print(\"Predicted Label:\", predicted_labels[0])\n",
    "\n",
    "# Check model function (as defined earlier)\n",
    "def check_model(model, input_data, scaler):\n",
    "    # Convert categorical variables to dummy variables\n",
    "    input_data_encoded = pd.get_dummies(input_data, drop_first=True)\n",
    "\n",
    "    # Ensure the same features are present\n",
    "    missing_cols = set(X.columns) - set(input_data_encoded.columns)\n",
    "    for col in missing_cols:\n",
    "        input_data_encoded[col] = 0  # Add missing columns with 0s\n",
    "\n",
    "    # Reorder columns to match training data\n",
    "    input_data_encoded = input_data_encoded[X.columns]\n",
    "\n",
    "    # Scale the features\n",
    "    input_data_scaled = scaler.transform(input_data_encoded)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(input_data_scaled)\n",
    "    predictions = np.round(predictions).astype(int)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "# Assuming global_model and scaler are available\n",
    "get_user_input_and_predict(global_model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ecae10-0a0f-44cd-823c-5c7ee4db4f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m y_splits \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(y_train, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Perform federated training\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m global_model \u001b[38;5;241m=\u001b[39m \u001b[43mfederated_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     56\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m global_model(X_test)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[18], line 44\u001b[0m, in \u001b[0;36mfederated_train\u001b[1;34m(X_splits, y_splits, num_rounds, dropout_rate, learning_rate)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mround_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_splits, y_splits):\n\u001b[1;32m---> 44\u001b[0m         \u001b[43mmaml_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m global_model\n",
      "Cell \u001b[1;32mIn[18], line 27\u001b[0m, in \u001b[0;36mmaml_train_step\u001b[1;34m(model, optimizer, X, y)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaml_train_step\u001b[39m(model, optimizer, X, y):\n\u001b[1;32m---> 27\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m preprocess_data(X, y)  \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     29\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m model(X)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_data(X, y, test_size=0.2, random_state=42):\n",
    "    # Convert to NumPy arrays if not already\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Updated maml_train_step function\n",
    "def maml_train_step(model, optimizer, X, y):\n",
    "    X, y = preprocess_data(X, y)  # Preprocess data\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X)\n",
    "        predictions = tf.squeeze(predictions)  # Ensure correct shape\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Federated Training Setup with Adjustments\n",
    "def federated_train(X_splits, y_splits, num_rounds=20, dropout_rate=0.2, learning_rate=0.0005):\n",
    "    global_model = AdvancedMAMLModel(dropout_rate=dropout_rate)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}\")\n",
    "        for X, y in zip(X_splits, y_splits):\n",
    "            maml_train_step(global_model, optimizer, X, y)\n",
    "    return global_model\n",
    "\n",
    "# Preprocess and split the dataset\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)  # Provide full dataset X, y\n",
    "X_splits = np.array_split(X_train, 5)\n",
    "y_splits = np.array_split(y_train, 5)\n",
    "\n",
    "# Perform federated training\n",
    "global_model = federated_train(X_splits, y_splits)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = global_model(X_test).numpy()\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612350d2-f062-41a1-8dde-4b21495a6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the MAML and Federated Learning Model script with fixes and improvements.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define updated MAML model with Dropout and Batch Normalization\n",
    "class AdvancedMAMLModel(tf.keras.Model):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(AdvancedMAMLModel, self).__init__()\n",
    "        self.fc1 = layers.Dense(64, activation='relu')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.fc2 = layers.Dense(32, activation='relu')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Preprocessing function to scale data\n",
    "def preprocess_data(X, y, scaler=None, fit_scaler=True):\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    \n",
    "    if fit_scaler and scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "    \n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Federated training setup\n",
    "def maml_train_step(model, optimizer, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X, training=True)\n",
    "        predictions = tf.squeeze(predictions)\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss.numpy()\n",
    "\n",
    "def federated_train(X_splits, y_splits, num_rounds=10, dropout_rate=0.5, learning_rate=0.001):\n",
    "    global_model = AdvancedMAMLModel(dropout_rate=dropout_rate)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}/{num_rounds}\")\n",
    "        for X, y in zip(X_splits, y_splits):\n",
    "            maml_train_step(global_model, optimizer, tf.convert_to_tensor(X, dtype=tf.float32), tf.convert_to_tensor(y, dtype=tf.float32))\n",
    "    \n",
    "    return global_model\n",
    "\n",
    "# Example data preprocessing and training pipeline\n",
    "def main():\n",
    "    # Example: Load and preprocess data (replace with actual dataset path)\n",
    "    data = pd.read_csv('blackhole.csv')\n",
    "    normal = data[data['label'] == 0]\n",
    "    blackhole = data[data['label'] == 1]\n",
    "    \n",
    "    # Upsample minority class\n",
    "    blackhole_upsampled = resample(blackhole, replace=True, n_samples=len(normal), random_state=42)\n",
    "    combined_data = pd.concat([normal, blackhole_upsampled])\n",
    "    \n",
    "    # Encode categorical features\n",
    "    encoded_data = pd.get_dummies(combined_data, drop_first=True)\n",
    "    X = encoded_data.drop('label', axis=1)\n",
    "    y = encoded_data['label']\n",
    "    \n",
    "    # Preprocess and split data\n",
    "    X_scaled, y, scaler = preprocess_data(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Simulate federated learning data\n",
    "    X_splits = np.array_split(X_train, 5)\n",
    "    y_splits = np.array_split(y_train, 5)\n",
    "    \n",
    "    # Train the global model\n",
    "    global_model = federated_train(X_splits, y_splits, num_rounds=10)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = global_model(tf.convert_to_tensor(X_test, dtype=tf.float32)).numpy()\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nAccuracy Score:\")\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Execute the pipeline\n",
    "# Uncomment the following line if running in an interactive environment\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35140f64-5188-48a7-809e-14abe37e6d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (404134, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Round 2\n",
      "Round 3\n",
      "Round 4\n",
      "Round 5\n",
      "Round 6\n",
      "Round 7\n",
      "Round 8\n",
      "Round 9\n",
      "Round 10\n",
      "Confusion Matrix:\n",
      "[[53962     1]\n",
      " [    0 53978]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     53963\n",
      "           1       1.00      1.00      1.00     53978\n",
      "\n",
      "    accuracy                           1.00    107941\n",
      "   macro avg       1.00      1.00      1.00    107941\n",
      "weighted avg       1.00      1.00      1.00    107941\n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "0.9999907356796769\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('blackhole.csv')\n",
    "print(f'Dataset shape: {data.shape}')\n",
    "\n",
    "# Data preprocessing: Upsampling minority class\n",
    "normal = data[data['label'] == 0]\n",
    "blackhole = data[data['label'] == 1]\n",
    "\n",
    "# Upsample the minority class\n",
    "blackhole_upsampled = resample(\n",
    "    blackhole, replace=True, n_samples=len(normal), random_state=42\n",
    ")\n",
    "balanced_data = pd.concat([normal, blackhole_upsampled])\n",
    "\n",
    "# One-hot encode categorical columns if any\n",
    "balanced_data_encoded = pd.get_dummies(balanced_data, drop_first=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = balanced_data_encoded.drop('label', axis=1)\n",
    "y = balanced_data_encoded['label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the enhanced MAML model\n",
    "class EnhancedMAMLModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(EnhancedMAMLModel, self).__init__()\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        self.fc2 = layers.Dense(64, activation='relu')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(0.3)\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Model training setup\n",
    "def maml_train_step(model, optimizer, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X, training=True)\n",
    "        predictions = tf.squeeze(predictions)  # Ensure predictions have the correct shape\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Federated Learning setup\n",
    "def federated_train(X_splits, y_splits, num_rounds=10):\n",
    "    global_model = EnhancedMAMLModel()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}\")\n",
    "        for X, y in zip(X_splits, y_splits):\n",
    "            maml_train_step(global_model, optimizer, X, y)\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Split data into 5 subsets for federated learning\n",
    "X_splits = np.array_split(X_train, 5)\n",
    "y_splits = np.array_split(y_train, 5)\n",
    "\n",
    "# Introduce EarlyStopping and Learning Rate Scheduler\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Federated training\n",
    "global_model = federated_train(X_splits, y_splits)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = global_model(X_test).numpy()\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e9ae1c-4328-471e-b894-982e4f8404f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (404134, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Round 1\n",
      "Validation Loss: 0.7123\n",
      "Round 2\n",
      "Validation Loss: 0.6673\n",
      "Round 3\n",
      "Validation Loss: 0.6238\n",
      "Round 4\n",
      "Validation Loss: 0.5824\n",
      "Round 5\n",
      "Validation Loss: 0.5417\n",
      "Round 6\n",
      "Validation Loss: 0.5012\n",
      "Round 7\n",
      "Validation Loss: 0.4612\n",
      "Round 8\n",
      "Validation Loss: 0.4212\n",
      "Round 9\n",
      "Validation Loss: 0.3817\n",
      "Round 10\n",
      "Validation Loss: 0.3432\n",
      "Confusion Matrix:\n",
      "[[39633   750]\n",
      " [    0 40573]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     40383\n",
      "           1       0.98      1.00      0.99     40573\n",
      "\n",
      "    accuracy                           0.99     80956\n",
      "   macro avg       0.99      0.99      0.99     80956\n",
      "weighted avg       0.99      0.99      0.99     80956\n",
      "\n",
      "\n",
      "Accuracy Score: 0.9907357082859826\n",
      "Precision: 0.9818503012849986\n",
      "Recall: 1.0\n",
      "F1-Score: 0.9908420435674514\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('blackhole.csv')\n",
    "print(f'Dataset shape: {data.shape}')\n",
    "\n",
    "# Data preprocessing\n",
    "normal = data[data['label'] == 0]\n",
    "blackhole = data[data['label'] == 1]\n",
    "\n",
    "# Upsample the minority class\n",
    "blackhole_upsampled = resample(\n",
    "    blackhole, replace=True, n_samples=len(normal), random_state=42\n",
    ")\n",
    "balanced_data = pd.concat([normal, blackhole_upsampled])\n",
    "\n",
    "# Encode categorical data and separate features and labels\n",
    "balanced_data_encoded = pd.get_dummies(balanced_data, drop_first=True)\n",
    "X = balanced_data_encoded.drop('label', axis=1)\n",
    "y = balanced_data_encoded['label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split with validation set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define a more structured MAML model\n",
    "class EnhancedMAMLModel(tf.keras.Model):\n",
    "    def __init__(self, hidden_units_1=128, hidden_units_2=64, dropout_rate=0.3):\n",
    "        super(EnhancedMAMLModel, self).__init__()\n",
    "        self.fc1 = layers.Dense(hidden_units_1, activation='relu')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.fc2 = layers.Dense(hidden_units_2, activation='relu')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.out(x)\n",
    "\n",
    "# Model training setup\n",
    "def maml_train_step(model, optimizer, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X, training=True)\n",
    "        predictions = tf.squeeze(predictions)\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Federated Learning setup with local training\n",
    "def federated_train(X_splits, y_splits, X_val, y_val, num_rounds=10, local_epochs=2):\n",
    "    global_model = EnhancedMAMLModel()\n",
    "    global_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    _ = global_model(tf.keras.Input(shape=(X_splits[0].shape[1],)))  # Build global model\n",
    "\n",
    "    # Validation tracking\n",
    "    val_losses = []\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}\")\n",
    "        \n",
    "        aggregated_weights = [np.zeros_like(w) for w in global_model.get_weights()]  # Initialize aggregated weights\n",
    "        for X, y in zip(X_splits, y_splits):\n",
    "            local_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            local_model = EnhancedMAMLModel()\n",
    "            _ = local_model(tf.keras.Input(shape=(X.shape[1],)))  # Build local model\n",
    "            local_model.set_weights(global_model.get_weights())  # Sync with global model\n",
    "\n",
    "            # Local training on each client\n",
    "            for _ in range(local_epochs):\n",
    "                maml_train_step(local_model, local_optimizer, X, y)\n",
    "\n",
    "            # Accumulate weights for aggregation\n",
    "            local_weights = local_model.get_weights()\n",
    "            for i, (global_w, local_w) in enumerate(zip(aggregated_weights, local_weights)):\n",
    "                aggregated_weights[i] += local_w / len(X_splits)\n",
    "\n",
    "        # Update global model weights after aggregation\n",
    "        global_model.set_weights(aggregated_weights)\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_predictions = global_model(X_val, training=False)\n",
    "        y_val_reshaped = tf.reshape(y_val, (-1, 1))  # Ensure y_val matches val_predictions\n",
    "        val_loss = tf.keras.losses.binary_crossentropy(y_val_reshaped, val_predictions).numpy().mean()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return global_model\n",
    "\n",
    "\n",
    "# Split training data for federated learning\n",
    "X_splits = np.array_split(X_train, 5)\n",
    "y_splits = np.array_split(y_train, 5)\n",
    "\n",
    "# Train the model\n",
    "global_model = federated_train(X_splits, y_splits, X_val, y_val)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = global_model(X_test, training=False).numpy()\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a4b1ee-7dcc-4770-8a9d-cee38666b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (404134, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Validation Loss: 0.7882\n",
      "Round 2\n",
      "Validation Loss: 0.7605\n",
      "Round 3\n",
      "Validation Loss: 0.7346\n",
      "Round 4\n",
      "Validation Loss: 0.7103\n",
      "Round 5\n",
      "Validation Loss: 0.6873\n",
      "Round 6\n",
      "Validation Loss: 0.6657\n",
      "Round 7\n",
      "Validation Loss: 0.6450\n",
      "Round 8\n",
      "Validation Loss: 0.6251\n",
      "Round 9\n",
      "Validation Loss: 0.6056\n",
      "Round 10\n",
      "Validation Loss: 0.5866\n",
      "Round 11\n",
      "Validation Loss: 0.5678\n",
      "Round 12\n",
      "Validation Loss: 0.5492\n",
      "Round 13\n",
      "Validation Loss: 0.5307\n",
      "Round 14\n",
      "Validation Loss: 0.5122\n",
      "Round 15\n",
      "Validation Loss: 0.4937\n",
      "Confusion Matrix:\n",
      "[[39447   936]\n",
      " [ 4183 36390]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94     40383\n",
      "           1       0.97      0.90      0.93     40573\n",
      "\n",
      "    accuracy                           0.94     80956\n",
      "   macro avg       0.94      0.94      0.94     80956\n",
      "weighted avg       0.94      0.94      0.94     80956\n",
      "\n",
      "\n",
      "Accuracy Score: 0.9367681209545926\n",
      "Precision: 0.9749236457161228\n",
      "Recall: 0.8969018805609642\n",
      "F1-Score: 0.9342867045790062\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('blackhole.csv')\n",
    "print(f'Dataset shape: {data.shape}')\n",
    "\n",
    "# Data preprocessing\n",
    "normal = data[data['label'] == 0]\n",
    "blackhole = data[data['label'] == 1]\n",
    "\n",
    "# Upsample the minority class\n",
    "blackhole_upsampled = resample(\n",
    "    blackhole, replace=True, n_samples=len(normal), random_state=42\n",
    ")\n",
    "balanced_data = pd.concat([normal, blackhole_upsampled])\n",
    "\n",
    "# Encode categorical data and separate features and labels\n",
    "balanced_data_encoded = pd.get_dummies(balanced_data, drop_first=True)\n",
    "X = balanced_data_encoded.drop('label', axis=1)\n",
    "y = balanced_data_encoded['label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split with validation set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define Enhanced MAML Model with Regularization and Dropout\n",
    "class EnhancedMAMLModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(EnhancedMAMLModel, self).__init__()\n",
    "        self.fc1 = layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.dropout1 = layers.Dropout(0.4)\n",
    "        self.fc2 = layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.dropout2 = layers.Dropout(0.4)\n",
    "        self.out = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# MAML Training Step\n",
    "def maml_train_step(model, optimizer, X, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X, training=True)\n",
    "        predictions = tf.squeeze(predictions)\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Federated Training Setup\n",
    "def federated_train(X_splits, y_splits, X_val, y_val, num_rounds=15, local_epochs=3):\n",
    "    global_model = EnhancedMAMLModel()\n",
    "    global_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008)\n",
    "    _ = global_model(tf.keras.Input(shape=(X_splits[0].shape[1],)))\n",
    "\n",
    "    val_losses = []\n",
    "\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}\")\n",
    "        \n",
    "        aggregated_weights = [np.zeros_like(w) for w in global_model.get_weights()]\n",
    "        for X, y in zip(X_splits, y_splits):\n",
    "            local_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "            local_model = EnhancedMAMLModel()\n",
    "            _ = local_model(tf.keras.Input(shape=(X.shape[1],)))\n",
    "            local_model.set_weights(global_model.get_weights())\n",
    "\n",
    "            # Local training on each client\n",
    "            for _ in range(local_epochs):\n",
    "                maml_train_step(local_model, local_optimizer, X, y)\n",
    "\n",
    "            # Accumulate weights for aggregation\n",
    "            local_weights = local_model.get_weights()\n",
    "            for i, (global_w, local_w) in enumerate(zip(aggregated_weights, local_weights)):\n",
    "                aggregated_weights[i] += local_w / len(X_splits)\n",
    "\n",
    "        # Update global model weights after aggregation\n",
    "        global_model.set_weights(aggregated_weights)\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_predictions = global_model(X_val, training=False)\n",
    "        y_val_reshaped = tf.reshape(y_val, (-1, 1))\n",
    "        val_loss = tf.keras.losses.binary_crossentropy(y_val_reshaped, val_predictions).numpy().mean()\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Split training data for federated learning\n",
    "X_splits = np.array_split(X_train, 5)\n",
    "y_splits = np.array_split(y_train, 5)\n",
    "\n",
    "# Train the model\n",
    "global_model = federated_train(X_splits, y_splits, X_val, y_val)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = global_model(X_test, training=False).numpy()\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f77b814-9d2b-4c8a-9726-d544f62c8f63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load your dataset (assuming text, audio features, and labels)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Sample dataset: 'text', 'audio_file_path', 'label'\u001b[39;00m\n\u001b[0;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmental_health_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import librosa\n",
    "\n",
    "# Load your dataset (assuming text, audio features, and labels)\n",
    "# Sample dataset: 'text', 'audio_file_path', 'label'\n",
    "data = pd.read_csv(\"mental_health_data.csv\")\n",
    "\n",
    "# Function to extract features from audio files\n",
    "def extract_audio_features(file_name):\n",
    "    y, sr = librosa.load(file_name, duration=30)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)\n",
    "    return mfcc\n",
    "\n",
    "# Extract audio features for all audio files\n",
    "data['audio_features'] = data['audio_file_path'].apply(extract_audio_features)\n",
    "\n",
    "# Text processing (TF-IDF vectorization)\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_text = vectorizer.fit_transform(data['text']).toarray()\n",
    "\n",
    "# Combine text and audio features\n",
    "X_audio = np.vstack(data['audio_features'].values)\n",
    "X_combined = np.hstack((X_text, X_audio))\n",
    "\n",
    "# Labels (assuming binary classification: 0 = no depression/suicidal tendency, 1 = depression/suicidal tendency)\n",
    "y = data['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the audio features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "# Training and evaluation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872406d-1fdd-4c37-8172-3408f521e1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
